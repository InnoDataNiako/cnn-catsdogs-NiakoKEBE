#  CNN From Scratch vs Transfer Learning - Cats vs Dogs

##  Objectif

Comparer deux approches de classification d'images sur le dataset Cats vs Dogs :
1. **CNN from scratch** : Architecture personnalis√©e entra√Æn√©e de z√©ro
2. **Transfer Learning** : Mod√®le pr√©-entra√Æn√© (MobileNetV2) adapt√© au probl√®me

##  Exp√©rimentations R√©alis√©es

-  4 mod√®les entra√Æn√©s (2 architectures √ó 2 optimiseurs)
-  R√©gularisation compl√®te (Dropout + Batch Normalization)
-  Data augmentation (flip, rotation, color jitter, affine)
-  Learning rate scheduling (StepLR et CosineAnnealing)
-  Early stopping (patience = 5 epochs)
-  M√©triques compl√®tes (Loss, Accuracy, Precision, Recall, F1-Score)
-  Matrices de confusion
-  Analyse comparative d√©taill√©e

---

##  Environnement Technique

### Configuration

- **Plateforme** : Google Colab
- **GPU** : Tesla T4 (16 GB VRAM)
- **PyTorch** : 2.8.0+cu126
- **TorchVision** : 0.23.0+cu126
- **Python** : 3.10+

### Installation des d√©pendances

```bash
pip install -r requirements.txt
```

---

##  Dataset

### Source

**Microsoft Cats vs Dogs Dataset**  
- Source : [Kaggle - Microsoft Cats vs Dogs](https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset)
- Taille totale : 788 MB
- Total images : 25,000 (12,500 chats + 12,500 chiens)

### Nettoyage

2 images corrompues d√©tect√©es et supprim√©es automatiquement :
- `/content/Cat_Dog_Data/train/cat/666.jpg`
- `/content/Cat_Dog_Data/test/dog/11702.jpg`

### R√©partition Finale

```
Dataset apr√®s nettoyage:
  ‚Ä¢ Train : 22,499 images (11,249 chats + 11,250 chiens)
    - Training : 19,124 images (85%)
    - Validation : 3,375 images (15%)
  ‚Ä¢ Test : 2,499 images (1,250 chats + 1,249 chiens)
```

### Data Augmentation

**Training Set** :
- Resize : 224√ó224
- RandomHorizontalFlip (p=0.5)
- RandomRotation (¬±15¬∞)
- ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2)
- RandomAffine (translate=0.1)
- Normalisation ImageNet

**Validation/Test Set** :
- Resize : 224√ó224
- Normalisation ImageNet uniquement

---

##  Architectures

### 1. CNN From Scratch

**Architecture** :
```
‚Ä¢ Bloc 1 : Conv(3‚Üí32) ‚Üí BN ‚Üí ReLU ‚Üí Conv(32‚Üí32) ‚Üí BN ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)
‚Ä¢ Bloc 2 : Conv(32‚Üí64) ‚Üí BN ‚Üí ReLU ‚Üí Conv(64‚Üí64) ‚Üí BN ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)
‚Ä¢ Bloc 3 : Conv(64‚Üí128) ‚Üí BN ‚Üí ReLU ‚Üí Conv(128‚Üí128) ‚Üí BN ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)
‚Ä¢ Bloc 4 : Conv(128‚Üí256) ‚Üí BN ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)
‚Ä¢ Global Average Pooling : 256 ‚Üí 1√ó1
‚Ä¢ Classifier : 
    - FC(256‚Üí512) ‚Üí BN ‚Üí ReLU ‚Üí Dropout(0.5)
    - FC(512‚Üí128) ‚Üí BN ‚Üí ReLU ‚Üí Dropout(0.5)
    - FC(128‚Üí2)
```

**Param√®tres** : 782,370 (tous entra√Ænables)

### 2. Transfer Learning (MobileNetV2)

**Architecture** :
- **Backbone** : MobileNetV2 pr√©-entra√Æn√© sur ImageNet (gel√©)
- **Classifier personnalis√©** :
  - Dropout(0.5)
  - FC(1280‚Üí512) ‚Üí BN ‚Üí ReLU
  - Dropout(0.5)
  - FC(512‚Üí2)

**Param√®tres** :
- Total : 2,881,794
- Entra√Ænables : 657,922 (22.8%)
- Gel√©s (backbone) : 2,223,872 (77.2%)

**Avantage** : 15.9% de param√®tres en moins √† entra√Æner par rapport au CNN from scratch

---

##  Configuration d'Entra√Ænement

### Hyperparam√®tres

| Param√®tre | Valeur |
|-----------|--------|
| Image Size | 224√ó224 |
| Batch Size | 64 |
| Epochs | 20 (avec early stopping) |
| Learning Rate | 0.001 |
| Dropout | 0.5 |
| Weight Decay | 0.0001 |
| Train/Val Split | 85/15 |
| Seed | 42 |

### Optimiseurs et Schedulers

| Mod√®le | Optimiseur | Scheduler | Converge en |
|--------|-----------|-----------|-------------|
| CNN From Scratch + SGD | SGD (momentum=0.9) | StepLR (step=7, Œ≥=0.1) | 12 epochs |
| CNN From Scratch + Adam | Adam | CosineAnnealingLR | 20 epochs |
| Transfer Learning + SGD | SGD (momentum=0.9) | StepLR (step=7, Œ≥=0.1) | 10 epochs |
| Transfer Learning + Adam | Adam | CosineAnnealingLR | 8 epochs |

---

##  R√©sultats

### Performances sur le Validation Set

| Mod√®le | Val Accuracy | Val Precision | Val Recall | Val F1 |
|--------|--------------|---------------|------------|--------|
| CNN From Scratch + SGD | **56.12%** | 95.60% | 13.56% | 23.70% |
| CNN From Scratch + Adam | **76.80%** | 82.35% | 79.25% | 75.26% |
| Transfer Learning + SGD | **97.19%** | 97.56% | 97.64% | 97.20% |
| **Transfer Learning + Adam** | **97.36%**  | **98.19%** | 97.64% | **97.38%** |

### Performances sur le Test Set

| Mod√®le | Test Accuracy | Test Precision | Test Recall | Test F1 |
|--------|---------------|----------------|-------------|---------|
| CNN From Scratch + SGD | **57.70%** | 93.64% | 16.49% | 28.05% |
| CNN From Scratch + Adam | **78.19%** | 79.88% | 75.34% | 77.54% |
| Transfer Learning + SGD | **97.20%** | 96.75% | 97.68% | 97.21% |
| **Transfer Learning + Adam** | **97.64%**  | **97.15%** | **98.16%** | **97.65%** |



##  Visualisations

### Exemples d'Images du Dataset

√âchantillon d'images avec data augmentation appliqu√©e (flip, rotation, color jitter) :

![Exemples d'images](images/sample_images.png)

---

### Courbes d'Entra√Ænement

####  Accuracy

![Courbes d'Accuracy](images/acc_curves.png)

**Observations** :
-  **Transfer Learning** converge d√®s les **3-5 premi√®res √©poques** (~97%)
-  **CNN From Scratch** n√©cessite **15-20 √©poques** pour atteindre 70-77%
-  **Adam** montre une convergence plus **stable et rapide** que SGD
-  √âcart de **30 points** entre CNN from scratch et Transfer Learning

####  Loss

![Courbes de Loss](images/loss_curves.png)

**Observations** :
-  Transfer Learning atteint rapidement un **loss faible** (<0.1)
-  CNN From Scratch + SGD stagne autour de **0.7** (convergence difficile)
-  Validation loss proche du training loss ‚Üí **pas d'overfitting**
-  L'√©cart train/val reste faible gr√¢ce √† Dropout + BN + Augmentation

####  Precision et Recall

![Courbes de Precision](images/precision_curves.png)

![Courbes de Recall](images/recall_curves.png)

**Observations** :
- **Transfer Learning** : Precision et Recall **√©quilibr√©s** (~97%)
- **CNN + Adam** : Bon √©quilibre (**~80%** sur les deux)
- **CNN + SGD** : **D√©s√©quilibre majeur** (Precision 95%, Recall 16%) ‚Üí biais fort

---

###  Matrices de Confusion

![Matrices de Confusion](images/confusion_matrices.png)

**Analyse d√©taill√©e** :

| Mod√®le | Vrais Positifs | Faux Positifs | Faux N√©gatifs | Observation |
|--------|----------------|---------------|---------------|-------------|
| **Transfer + Adam** | ~98% | ~2% | ~2% |  Excellent √©quilibre |
| **Transfer + SGD** | ~97% | ~3% | ~3% |  Tr√®s bonnes performances |
| **CNN + Adam** | ~75% | ~25% | ~25% |  Performances moyennes |
| **CNN + SGD** | ~16% | ~84% | ~3% |  Biais fort vers "Cat" |

**Probl√®me du CNN + SGD** : Le mod√®le pr√©dit **"Cat"** dans 84% des cas, m√™me pour des chiens ! C'est un cas typique de **convergence vers un minimum local**.

---

###  Comparaison Finale des 4 Mod√®les

![Comparaison Globale](images/final_comparison.png)

Cette visualisation synth√©tise les performances sur **toutes les m√©triques** :

**Classement final** :
1.  **Transfer Learning + Adam** : 97.64% (champion absolu)
2.  **Transfer Learning + SGD** : 97.20% (excellent)
3.  **CNN From Scratch + Adam** : 78.19% (honorable)
4.  **CNN From Scratch + SGD** : 57.70% (insuffisant)

**Conclusion visuelle** : Le Transfer Learning est **largement sup√©rieur** (+40 points d'accuracy) pour ce type de probl√®me avec un dataset de taille moyenne.

###  Meilleur : Transfer Learning + Adam

- **Test Accuracy** : 97.64%
- **Test F1-Score** : 97.65%
- **Convergence** : 8 epochs (avec early stopping)
- **Temps d'entra√Ænement** : ~17 minutes

---

##  Analyse Comparative

### CNN From Scratch vs Transfer Learning

| Aspect | CNN From Scratch | Transfer Learning |
|--------|------------------|-------------------|
| **Accuracy moyenne (Test)** | 67.95% | **97.42%** |
| **Am√©lioration** | - | **+43.37%**  |
| **Temps de convergence** | 12-20 epochs | 5-10 epochs |
| **Robustesse** | Sensible √† l'initialisation | Tr√®s robuste |
| **Donn√©es n√©cessaires** | Beaucoup (>50K) | Mod√©r√© (~20K) |

### SGD vs Adam

| Aspect | SGD | Adam |
|--------|-----|------|
| **Accuracy moyenne (Test)** | 77.45% | **87.92%** |
| **Am√©lioration avec Adam** | - | **+13.51%** |
| **Convergence** | Plus lente, instable | Rapide et stable |
| **Tuning requis** | Important | Minimal |

---

##  Observations Cl√©s

### 1. Sup√©riorit√© du Transfer Learning

 Le Transfer Learning **converge 2-3√ó plus rapidement** que le CNN from scratch  
 Performances finales **30-40% sup√©rieures** (~97% vs ~68%)  
 Le mod√®le pr√©-entra√Æn√© poss√®de d√©j√† des features g√©n√©riques efficaces  
 N√©cessite **moins de donn√©es** pour atteindre de bonnes performances  

### 2. Efficacit√© de l'optimiseur Adam

 Adam **+20% plus performant** que SGD sur CNN from scratch  
 Convergence **automatique et stable** du learning rate  
 Moins sensible √† l'initialisation des poids  
 Particuli√®rement efficace sur des architectures complexes  

### 3. Impact de la R√©gularisation

 **Dropout + Batch Normalization** ont √©vit√© l'overfitting  
 Data augmentation a significativement am√©lior√© la robustesse  
 Early stopping a √©conomis√© du temps de calcul  

### 4. Probl√®me du CNN From Scratch + SGD

 **Accuracy de seulement 57.70%** sur le test set  
 **Tr√®s faible Recall (16.49%)** : le mod√®le pr√©dit presque toujours "Cat"  
 Probl√®me de **convergence** : stuck dans un minimum local  
 SGD avec CNN from scratch **n√©cessite un tuning tr√®s fin** du learning rate  

---

##  Conclusions

### Principales Le√ßons

1. **Pour des datasets de taille moyenne (<100K images)** : Le Transfer Learning est **fortement recommand√©**
   - Gain de temps consid√©rable
   - Performances bien sup√©rieures
   - Moins de donn√©es n√©cessaires

2. **Choix de l'optimiseur** : Adam est **pr√©f√©rable √† SGD** pour ce type de probl√®me
   - Convergence automatique
   - Moins de hyperparam√®tre tuning
   - R√©sultats plus stables

3. **Importance de la r√©gularisation** : Dropout + BN + Data Augmentation sont **essentiels**
   - Pr√©vient l'overfitting
   - Am√©liore la g√©n√©ralisation
   - Rend le mod√®le plus robuste

4. **Early stopping** : Permet d'**√©conomiser du temps** sans d√©grader les performances
   - Evite le surapprentissage
   - Arr√™te l'entra√Ænement au bon moment

### Limites du Projet

- Dataset relativement **simple** (2 classes, images claires)
- Backbone **gel√©** : le fine-tuning complet pourrait am√©liorer davantage
- Hyperparam√®tres **non exhaustivement explor√©s** (pas de grid search)
- Une seule ex√©cution par configuration (pas de cross-validation)


##  Utilisation

### Reproduction des R√©sultats

Le projet a √©t√© d√©velopp√© sur **Google Colab** avec GPU Tesla T4.

```bash
1. Ouvrir le notebook sur Google Colab
2. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)
3. Ex√©cuter toutes les cellules s√©quentiellement
4. Les r√©sultats sont reproductibles (seed=42)
```

**Note** : Avec le m√™me seed, vous obtiendrez les m√™mes r√©sultats (¬±0.5% de variance due au GPU)

### Entra√Ænement Local

```bash
# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer Jupyter
jupyter notebook notebook.ipynb
```

---

## üì¶ Structure du D√©p√¥t

```
cnn-catsdogs-deeplearning/
‚îú‚îÄ‚îÄ notebook.ipynb          # Notebook Colab complet
‚îú‚îÄ‚îÄ .gitignore              # Fichiers √† ignorer
‚îú‚îÄ‚îÄ requirements.txt        # D√©pendances Python
‚îî‚îÄ‚îÄ README.md               # Ce fichier
```

**Note** : Les donn√©es (788 MB) et mod√®les (45 MB total) ne sont pas inclus dans le d√©p√¥t pour respecter les limites de GitHub.

---

##  R√©f√©rences

- **Dataset** : [Microsoft Cats vs Dogs](https://www.microsoft.com/en-us/download/details.aspx?id=54765)
- **MobileNetV2** : Sandler et al., 2018 - [arXiv:1801.04381](https://arxiv.org/abs/1801.04381)
- **PyTorch Documentation** : [https://pytorch.org/docs/](https://pytorch.org/docs/)
- **Transfer Learning Guide** : [https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)

---

##  Auteur

**Deep Learning Project**  
Cours de Deep Learning - Octobre 2025  
Plateforme : Google Colab (Tesla T4 GPU)

---

##  Licence

Ce projet est r√©alis√© √† des fins √©ducatives dans le cadre d'un cours universitaire.

---

##  Remerciements

Merci au professeur Moussa DIALLO  diallomous@gmail.com pour ce projet passionnant qui nous a permis de comprendre concr√®tement :
- L'architecture des CNN
- L'importance du Transfer Learning
- L'impact du choix de l'optimiseur
- Les techniques de r√©gularisation

---

** Transfer Learning + Adam : 97.64% d'accuracy - Mission accomplie ! **